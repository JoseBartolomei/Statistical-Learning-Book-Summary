Introduction to Statistical Learning Book Summary: Chapter 8
========================================================
author: 
date: 

Chapter 8. Tree-Based Methods
========================================================

Tree-Based Methods
========================================================
- These involve stratifying or segmenting the predictor space into a number of simple regions.
- Tree-based methods are simple and useful for interpretation. 
- They typically are not competitive with the best supervised learning approaches.
- The most powerfull of these are **Random Forest** and **Boosting**.
	- These approaches involves producing multiple trees which are then combined to yield a single consensus prediction.
	- Combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation.
	
	
Tree Terminology
========================================================
- Terminal Nodes or leaves of the tree: The regions of regression splits.
- Internal Nodes: The points along the tree where the predictor space is split.
- Branches: The segments of the trees that connect the nodes.
- In a regressio tree the tree is upside down.

Prediction via Stratification of the Feature Space
========================================================
Two steps procedure to build a tree:

1. Divide the predictor space—that is, the set of possible values for $X_1, X_2, ..., X_p$ into J distinct and non-overlapping regions, $R_1, R_2, ..., R_J$.

2. For every observation that falls into the region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.

Tree Step 1: construct the regions
========================================================
- The goal is to find boxes $R_1, R_2, ..., R_J$ that minimize the RSS, given by

$$\sum^{J}_{j=1}\sum_{i \in R_j}(y_i - \hat{y}_{R_{j}})^2$$, where

$\hat{y}_{R_{j}}$ is the mean response for the training observations within the $j^{th}$ box.

Tree Pruning
========================================================
- Big trees may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance.
- A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.
	- An alternative is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.
	- A better strategy is to grow a very large tree $T_0$, and then prune it back in order to obtain a subtree.
		- The goal is to select a subtree that leads to the lowest test error rate.
		
Cost complexity pruning
========================================================
- The is needed to estimate the test error on the modeled trees.
	- Estimating the cross-validation error for every possible subtree would be too cumbersome, since there is an extremely large number of possible subtrees.
- Cost complexity pruning or Weakest link pruning consider a sequence of trees indexed by a nonnegative tuning parameter $\alpha$.

Continuation on Tree
========================================================

Classification Trees
========================================================

A **classification tree** is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. 

Recall that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. 

In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. 

- In interpreting the results of a classification tree, we are often interested not only in the class prediction corresponding to a particular terminal node region, but also in the class proportions among the training observations that fall into that region.

Trees Versus Linear Models
========================================================
- Which model is better?
	- If the
relationship between the features and the response is well approximated by a linear model then an approach such as linear regression will likely work well, and will outperform a method such as a regression tree that does not exploit this linear structure.

	- If instead there is a highly non-linear and complex relationship between the features and the response then decision trees may outperform classical approaches.
	
- The relative performances of tree-based and classical approaches can be assessed by estimating the test error, using either cross-validation or the validation set approach.
- Prediction using a tree may be preferred for the sake of interpretability and visualization.

Advantages and Disadvantages of Trees
========================================================
Advantages

- Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression.
- Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.
- Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).
- Trees can easily handle qualitative predictors without the need to create dummy variables.

Disadvantages
- Trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches.

Advance Tree Aproahces
========================================================
- Bagging, random forests, and boosting use trees as building blocks to construct more powerful prediction models.

- Baggig will be described to present concepts which are based  Random Forest and Boosting which are more powerfull aproaches than Bagging.

Bootstrap aggregation or Bagging
========================================================
- Bootstrap aggregation or Bagging is a general-purpose procedure for reducing the variance of a statistical learning method.

- A natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions.

$$\hat{f}_{avg}(x) = \frac{1}{B}\sum^{B}_{b=1}\hat{f}^{b}(x)$$

Bagging
========================================================

- As we generally do not have access to multiple training sets, we can bootstrap, by taking repeated samples from the (single) training data set.
	
- Then train our method on the $b^{th}$ bootstrapped training set in order to get $\hat{f}^{*b}(x)$ and finally average all the predictions.

$$\hat{f}_{bag}(x) = \frac{1}{B}\sum^{B}_{b=1}\hat{f}^{*b}(x)$$

Bagging on Regression Tree
========================================================
To apply bagging to regression trees:

1. 	Cnstruct $B$ regression trees using $B$ bootstrapped  training sets, and average the resulting predictions. 
	- These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. 
	- Averaging these $B$ trees reduces the variance. 
	- Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.
	
Bagging on Classification Regression Tree
========================================================

- For a given test observation, we can record the class predicted by each of the $B$ trees, and take a majority vote: the overall prediction is the most commonly occurring class among the $B$ predictions.

Out-of-Bag Error Estimation
========================================================

- Method to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach.

- One-third of the observations that are not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations.

- The response for the $i^{th}$ observation can be predicted using each of the trees in which that observation was OOB.

OOB
========================================================
In order to obtain a single prediction for the ith observation, we can average these predicted responses (if regression is the goal) or can take a majority vote (if classification is the goal). This leads to a single OOB prediction for the ith observation. 

An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. 

The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation.

Bagging predictor importance
========================================================
- An overall summary of the importance of each predictor can be obtained using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees).

Random Forest
========================================================
- Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees.

- When building these decision trees:
	- each time a split in a tree is considered
	- a random sample of m predictors is chosen as split candidates from the full set of p predictors.
	- the split is allowed to use only one of those m predictors.
	- a fresh sample of m predictors is taken at each split, $m \approx \sqrt{p}$ is chosen—that is,
		- the number of predictors considered at each split is approximately equal to the square root of the total number of predictors.
		
Random Forest (RF)
========================================================
- Averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.

- RF decorrelate the tress by considereing only a subset of predictors.
	- On average $(p − m)/p$ of the splits will not consider the strong predictor, and so other predictors will have more of a chance.
	- Using a small value of $m$ in building a random forest will typically be helpful when we have a large number of correlated predictors.