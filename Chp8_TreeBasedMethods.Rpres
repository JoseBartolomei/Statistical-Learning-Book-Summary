Introduction to Statistical Learning Book Summary: Chapter 8
========================================================
author: 
date: 

Chapter 8. Tree-Based Methods
========================================================

Tree-Based Methods
========================================================
- These involve stratifying or segmenting the predictor space into a number of simple regions.
- Tree-based methods are simple and useful for interpretation. 
- They typically are not competitive with the best supervised learning approaches.
- The most powerfull of these are **Random Forest** and **Boosting**.
	- These approaches involves producing multiple trees which are then combined to yield a single consensus prediction.
	- Combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation.
	
	
Tree Terminology
========================================================
- Terminal Nodes or leaves of the tree: The regions of regression splits.
- Internal Nodes: The points along the tree where the predictor space is split.
- Branches: The segments of the trees that connect the nodes.
- In a regressio tree the tree is upside down.

Prediction via Stratification of the Feature Space
========================================================
Two steps procedure to build a tree:

1. Divide the predictor spaceâ€”that is, the set of possible values for $X_1, X_2, ..., X_p$ into J distinct and non-overlapping regions, $R_1, R_2, ..., R_J$.

2. For every observation that falls into the region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.

Tree Step 1: construct the regions
========================================================
- The goal is to find boxes $R_1, R_2, ..., R_J$ that minimize the RSS, given by

$$\sum^{J}_{j=1}\sum_{i \in R_j}(y_i - \hat{y}_{R_{j}})^2$$, where

$\hat{y}_{R_{j}}$ is the mean response for the training observations within the $j^{th}$ box.

Tree Pruning
========================================================
- Big trees may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance.
- A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.
	- An alternative is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.
	- A better strategy is to grow a very large tree $T_0$, and then prune it back in order to obtain a subtree.
		- The goal is to select a subtree that leads to the lowest test error rate.
		
Cost complexity pruning
========================================================
- The is needed to estimate the test error on the modeled trees.
	- Estimating the cross-validation error for every possible subtree would be too cumbersome, since there is an extremely large number of possible subtrees.
- Cost complexity pruning or Weakest link pruning consider a sequence of trees indexed by a nonnegative tuning parameter $\alpha$.

Continuation on Tree
========================================================

Classification Trees
========================================================

A **classification tree** is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. Recall that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. In interpreting the results of a classification tree, we are often interested not only in the class prediction corresponding to a particular terminal node region, but also in the class proportions among the training observations that fall into that region.

Trees Versus Linear Models
========================================================
- Which model is better?
	- If the
relationship between the features and the response is well approximated by a linear model then an approach such as linear regression will likely work well, and will outperform a method such as a regression tree that does not exploit this linear structure.

	- If instead there is a highly non-linear and complex relationship between the features and the response then decision trees may outperform classical approaches.
	
- The relative performances of tree-based and classical approaches can be assessed by estimating the test error, using either cross-validation or the validation set approach.
- Prediction using a tree may be preferred for the sake of interpretability and visualization.

Advantages and Disadvantages of Trees
========================================================
Advantages

- Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression.
- Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.
- Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).
- Trees can easily handle qualitative predictors without the need to create dummy variables.

Disadvantages
- Trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches.

Advance Tree Aproahces
========================================================
- Bagging, random forests, and boosting use trees as building blocks to construct more powerful prediction models.
- Baggig will be describe to present the concepts which are based this methods but Random Forest and Boosting are more powerfull aproaches than Bagging.

Bagging
========================================================