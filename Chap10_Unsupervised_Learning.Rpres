Chap 10 Unsupervised Learning (UL)
========================================================
author: 
date: 

What is UL?
========================================================

- Unsupervised learning is a set of statistical tools intended for the setting in which we have only a set of features $X_1, X_2,..., X_p$ measured on $n$ observations.
- There is no response, $Y$, variable.
- Furthermore, there is no interest in prediction, because we do not have an associated response variable $Y$. 
- The goal is to discover interesting things about the measurements on $X_1, X_2,..., X_p.$
- UL address the following questions:
	- Is there an informative way to visualize the data? 
	- Can we discover subgroups among the variables or among the observations?
- Unsupervised learning is often performed as part of an exploratory data analysis.
	
The Challenge of Unsupervised Learning
========================================================
- Tends to be more subjective
- There is no simple goal for the analysis, such as prediction of a response.
- It can be hard to assess the results obtained from unsupervised learning methods, since there is no universally accepted mechanism for performing cross-validation or validating results on an independent data set.

Principal Components (PC) Analysis
========================================================
- Principal components allow us to summarize this set of correlated variables with a smaller number of representative variables that collectively explain most of the variability in the original set.

- PC results are use as predictors in a regression model in place of the original larger set of variables.

- Principal component analysis (PCA) refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data.

- PCA can be used as a data visualization technique. Is a better method to visualize the $n$ observations when $p$ is large.

Principal Components (PC) Analysis
========================================================
- PCA help find a low-dimensional representation of the data that captures as much of the information as possible or in other words...
	- finds a low-dimensional representation of a data set that contains as much as possible of the variation.

The idea behind PCA
========================================================
- Each of the $n$ observations lives in p-dimensional space, but not all of these dimensions are equally interesting. 
- PCA seeks a small number of dimensions that are as interesting as possible.
	- The concept of interesting is measured by the amount that the observations vary along each dimension. 
- Each of the dimensions found by PCA is a linear combination of the $p$ features.

How to obtain PCA's
========================================================
- The first principal component of a set of features $X_1, X_2, ..., X_p$ is the normalized linear combination of the features hat has the largest variance.

$$Z_1 = \phi_{11}X_1 + \phi_{21}X_2 + ... + \phi_{p1}X_p$$ 

, where
- normalize = $\sum^{p}_{j=1} \phi^{2}_{j1} = 1$
- $\phi_{p1}...$ are the loadings of the first principal component.

- The loadings ($\phi$) are set to one because on the contrary could result in an arbitrarily large variance.

- The loading vector $\phi_1$ with elements $\phi_{11}, \phi_{21}, ..., \phi_{p1}$ defines a direction in feature space along which the data vary the most.

- For further mathematical explanation refer to the Element of Statistical Learning book

Another Interpretation of Principal Components
========================================================

- In previous slides was describe the principal component loading vectors as the directions in feature space along which the data vary the most, and the principal component scores as projections along these directions.

- An alternative interpretation for principal components is: principal components provide low-dimensional linear surfaces that are closest to the observations.

- The first principal component loading vector has a very special property: it is the line in p-dimensional space that is closest to the n observations.

Scale Variables
========================================================

- Variables needs to be scaled before perform PCA because scaling have a substantial effect on the results obtained.

- If we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for the variable with the highest variance.

- Because it is undesirable for the principal components obtained to depend on an arbitrary choice of scaling, is recommended to scale each variable to have standard deviation of one before we perform PCA.

- If variables are measured in the same units then is not necessary to scale the variables to have standard deviation one before performing PCA.

Proportion of Variance Explained
========================================================

- How much of the information in a given data set is lost by projecting the observations onto the first few principal components? 

or 

- How much of the variance in the data is not contained in the first few principal components?

Proportion of Variance Explained
========================================================

- The total variance presented in the data is defined as:

$$\sum^{p}_{j=1}Var(X_j) = \sum^{p}_{j=1}\frac{1}{n}\sum^{n}_{i=1}x^{2}_{ij}$$

- The variance explained by the $m^{th}$ principal component is defined as:

$$\frac{1}{n}\sum^{n}_{i=1}z^{2}_{im} = \frac{1}{n}\sum^{n}_{i=1}\Bigg(\sum^{p}_{j=1}\phi_{jm}x_{ij}\Bigg)^2$$

Proportion of Variance Explained
========================================================

- Therefore, the PVE of the $m^{th}$ principal component is given by:

$$\frac{\sum^{n}_{i=1}\big(\sum^{p}_{j=1}\phi_{jm}x_{ij})^2}{\sum^{p}_{j=1}\sum^{n}_{i=1}x^{2}_{ij}}$$

- The PVE of each principal component is a positive quantity.

- In total, there are $min(n âˆ’ 1, p)$ principal components, and their PVEs sum to one.

- A scree plot is use to visualize the PVE.

Deciding How Many Principal Components to Use
========================================================

- The ideal is to use the smallest number of principal components required to get a good understanding of the data.

- Typically, decide on the number of principal components required to visualize the data by examining a scree plot.
	- By eyeballing the scree plot, look for a point at which the proportion of variance explained by each subsequent principal component drops off.
	
	- Usually if no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest.
	
Deciding How Many Principal Components to Use on Supervise Learning
========================================================

- The number of principal component score vectors can be used in the regression as a tuning parameter to be selected via cross-validation or a related approach.

Clustering Methods
========================================================
- Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set.
	- The method seek to partition the data into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.

- Clustering vs PCA
	- PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance.
	- Clustering looks to find homogeneous subgroups among the observations.
	
Type of Clustering Methods
========================================================
- There are many type of methods. The two specified in the book are k-mean clustering and hierarchical clustering
	- K-means clustering: The mathematical algorithm seek to partition the observations into a pre-specified clustering number of clusters.
	- Hierarchical clustering: In this algorithm is not know in advance how many clusters will be obtain. The result of the procure is a tree-like visual representation of the observations, called a dendrogram, q visual representation that allows a view of the clustering obtained for each possible number of clusters, from $1$ to $n$.
	
k-Means Clustering
========================================================
- K-means clustering is an approach for partitioning a data set into $K$ distinct, non-overlapping clusters.
- To perform K-means clustering:
	- first, specify the desired number of clusters $K$
	- second, then the K-means algorithm will assign each observation to exactly one of the $K$.
	
k-Means Clustering theoretical procedure
========================================================
- Notation: $C_1,...,C_K$ denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:

1. $C_1 \cup C_2 \cup ... \cup C_K = \{1,...,n\}$

	- Each observation belongs to at least one of the $K$ clusters.

2. $C_k \cap C_{k'} = \emptyset$ for all $K \neq K'$ 

	- The clusters are non-overlapping: no observation belongs to more than one cluster.

k-Means Clustering Math
========================================================
- The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible.

- The within-cluster variation for cluster $C_k$ is a measure $W(C_k)$ of the amount by which the observations within a cluster differ from each other.

$$minimize\bigg\{\sum^{K}_{k=1}\frac{1}{C_k}W(C_k)\bigg\}$$

- In words, we want to partition the observations into $K$ clusters such that the total within-cluster variation, summed over all $K$ clusters, is as small as possible.

k-Means Clustering Math
========================================================

- $W(C_k)$, within-cluster variation,  can be defined in many ways but the most common choice is the squared Euclidean distance

$$W(C_k) = \frac{1}{|C_k|}\sum_{i,i' \in C_k}\sum^{p}_{j=1}(x_{ij}-x_{ij})^2$$

- where $|C_k|$ denotes the number of observations in the $k^{th}$ cluster.

- The within-cluster variation for the $k^{th}$ cluster is the sum of all of the pair wise squared Euclidean distances between the observations in the kth cluster, divided by the total number of observations in the kth cluster.

k-Means Clustering Math
========================================================
- Furthermore, the K-means clustering optimization problem is define as:

$$minimize\bigg\{\sum^{K}_{k=1}\frac{1}{|C_k|}\sum_{i,i' \in C_k}\sum^{p}_{j=1}(x_{ij}-x_{ij})^2\bigg\}$$


k-Means Clustering Algorithm to solve optimization problem
========================================================
1. Randomly assign a number, from 1 to K, to each of the observations. These serve as initial cluster assignments for the observations.

2. Iterate until the cluster assignments stop changing:
	- For each of the K clusters, compute the cluster centroid. The kth cluster centroid is the vector of the p feature means for the observations in the kth cluster.
	- Assign each observation to the cluster whose centroid is closet.

k-Means Clustering Algorithm to solve optimization problem
========================================================
- As the algorithm is run, the clustering obtained will continually improve until the result no longer changes. When the result no longer changes, a local optimum has been reached.

- The best solution is the one with the smallest optimization value.
- One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters K.

Hierarchical Clustering
========================================================
- Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of K.

- Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.

Bottom-up or agglomerative clustering
========================================================
- This is the most common type of hierarchical clustering, and refers to the fact that a dendrogram (generally depicted as an upside-down tree).

Interpreting a Dendrogram
========================================================
- Each leaf of the dendogram represents the observation in the data.
- The leaf that fuse into branches are observations similar to each others.
- As is move higher up the tree, branches themselves fuse, either with leaves or other branches. 
	- The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other. 
	- Observations that fuse later (near the top of the tree) can be quite different.
- The height of the fusion, as measured on the vertical axis, indicates how different the two observations are.

Interpreting a Dendrogram
========================================================

- there are $2^{nâˆ’1}$ possible reordering of the dendrogram, where $n$ is the number of leaves. This is because at each of the $n âˆ’ 1$ points where fusions occur, the positions  of the two fused branches could be swapped without affecting the meaning of the dendrogram. 

- Therefore, we cannot draw conclusions about the similarity of two observations based on their proximity along the horizontal axis. Rather, we draw conclusions about the similarity of two observations based on the location on the vertical axis where branches containing those two observations first are fused.

Identifying Clusters
========================================================

- Make a horizontal cut across the dendrogram.
- The height of the cut to the dendrogram serves the same role as the K in K-means clustering: it controls the number of clusters obtained.
- Often the choice of where to cut the dendrogram is not so clear.
- The term hierarchical refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height.

Identifying Clusters
========================================================
- Hierarchical clustering can sometimes yield worse (i.e. less accurate) results than K-means clustering for a given number of clusters.
	- This can happen when true clusters are not nested, in the sense that the best division into three groups does not result from taking the best division into two groups and splitting up one of those groups.
	
Hierarchical Clustering Algorithm 
========================================================
1. Begin with $n$ observations and a measure (such as Euclidean distance) of all the $\binom{n}{2} = n(n âˆ’ 1)/2$ pairwise dissimilarities. Treat each observation as its own cluster.

2. For $i = n, n âˆ’ 1,..., 2$:

- Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.
- Compute the new pairwise inter-cluster dissimilarities among the $i âˆ’ 1$ remaining clusters.

Clustering Linkage
========================================================

- Linkage: Defines the dissimilarity between two groups of observations.
	- The four most common types of linkageâ€”complete, average, single, and centroid.

A summary of the four most commonly used types of linkage in hierarchical clustering
========================================================

Linkage  | Description
------------- | -------------
Complete  | Maximal inter cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of the dissimilarities.
Single  | Minimal inter cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.
Average | Mean inter cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.
Centroid | Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.

Choice of Dissimilarity Measure
========================================================

- Euclidean distance
- Correlation-based distance: considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.
- To select a dissimilarity measure careful attention should be paid to the type of data being clustered and the scientific question at hand.

Small Decisions with Big Consequences
========================================================
In order to perform clustering, some decisions must be made.

- Should the observations or features first be standardized in some way?

- In the case of hierarchical clustering, 
	- What dissimilarity measure should be used?
	- What type of linkage should be used?
	- Where should we cut the dendrogram in order to obtain clusters?

- In the case of K-means clustering, how many clusters should we look for in the data?

Validating the Clusters Obtained
========================================================

There exist a number of techniques for assigning a p-value to a cluster in order to assess whether there is more evidence for the cluster than one would expect due to chance. However, there has been no consensus on a single best approach.