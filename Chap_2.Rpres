Introduction to Statistical Learning Book Summary
========================================================
author: 
date: 

Chapter 2
========================================================

- There are two main reasons that we may wish to estimate f : prediction and inference.

- Inference

Prediction
========================================================
- Prediction: We want to obtain the unknown value of an output ($Y$) using the known values of inputs ($X's$)
	- $$\hat{Y} = \hat{f}(X)$$

- where 
	- $\hat{f}$ represents our estimate for $f$
	- $\hat{Y}$ represents the resulting prediction for Y .

Prediction
========================================================
$$E(Y-\hat{Y})^2 = [f(X) - \hat{f}(X)^2] + Var(\epsilon)$$

- The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on two quantities,
	- reducible error: This error is reducible because we can potentially improve the accuracy of $\hat{f}$ by using the most appropriate statistical learning technique to estimate $f$.
	- irreducible error: Y is also a function of $\epsilon$, which, by definition, cannot be predicted using X. Therefore, variability associated with also affects the accuracy of our predictions.
		- No matter how well we estimate $f$, we cannot reduce the error introduced by $\epsilon$.
		
Inference
========================================================
- In inference we want to understand the relationship between $X$ and $Y$, or more specifically, to understand how $Y$ changes as a function of $X_1 , . . . , X_p$.

- For inference we may ask:
	- Which predictors are associated with the response?
	- What is the relationship between the response and each predictor?
	- Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?
	
How Do We Estimate f?
========================================================

$f$ is estimated by parametric or non-parametric functions

Parametric
========================================================
Parametric methods involve a two-step approach

1. Make an assumption about the functional form, or shape, of $f$ (eg., linear, quadratic, etc).

2. Find a procedure that uses the training data to fit or train the model (eg., ordinary least square, maximum likelihood, etc).

Non-Parametric
========================================================

- Non-parametric methods do not make explicit assumptions about the functional form of $f$. Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly.

- Advantages: by avoiding the assumption of a particular functional form for $f$, they have the potential to accurately fit a wider range of possible shapes for $f$. Any parametric approach brings with it the possibility that the functional form used to estimate f is very different from the true $f$, in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of $f$ is made.

Non-Parametric disadvantage
========================================================

Since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more
than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$.

The Trade-Off Between Prediction Accuracy and Model Interpretability
========================================================

- Some models are less flexible or restrictive, in the sense that they can produce just a relatively small range of shapes to estimate $f$.
- Flexible vs non-flexible
	- interested in inference, then restrictive models are much more interpretable.
	- If prediction is of interest less interpretable but more flexible and accurate models are useful.
	
Supervised Versus Unsupervised Learning
========================================================
- Supervised: For each observation of the predictor measurement(s) $x_i , i = 1, . . . , n$ there is an associated response measurement $y_i$ .
	- We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference).
	- examples: linear or logistic regression, Generalized Additive Model, boosting, support vector machines.

Supervised Versus Unsupervised Learning
========================================================
- Unsupervised: For every observation $i = 1, . . . , n$, we observe a vector of measurements $x_i$ but no associated response $y_i$.
- Lack a response variable that can supervise our analysis.
- With this analysis is possible to understand the relationship between variables or between the observations.
	- examples: cluster analysis, clustering

Regression Versus Classification Problems
========================================================
- Regression: Usually used when the response is quantitative
- Classification: Usually used when the response is qualitative

However, the distinction is not always that crisp at the moment of select a statistical methods because some models can be extended to work with both, quantitative and qualitative response.

Assessing Model Accuracy: Measuring the Quality of Fit
========================================================
When assessing the quality of the fit we want to:

1. measure how well its predictions actually match the observed data.
2. quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation.
3. The most common measure is the mean squared error (MSE)

$$MSE = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{f}(x_i))^2$$

MSE
========================================================
The MSE will be **small** if the predicted responses are very close to the true responses, and will be **large** if for some of the observations, the predicted and true responses differ substantially.

- we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.

- We want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE.

- If test data is available, implement learning methods to the test data and select the model with the small value of MSE.

MSE
========================================================

- Degrees of freedom is the average training MSE as a function of flexibility. The degrees of freedom is a quantity that summarizes the flexibility of a curve.

-  As model flexibility increases, training MSE will decrease, but the test MSE may not.

- This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. 
- When a given method yields a small training MSE but a large test MSE, we are said to be over-fitting the data.

MSE
========================================================
- This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function $f$.

- When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply donâ€™t exist in the test data.

- Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.

The Bias-Variance Trade-Off
========================================================
- The U-shape usually observed in a test MSE curves turns out to be the result of two competing properties of statistical learning methods: Bias and Variance.

- Expected test MSE, for a given value $x_0$, can always be decomposed into the sum of three fundamental quantities: 
	- the variance of $\hat{f}(x_0)$,
	- the squared bias of $\hat{f}(x_0)$
	- the variance of the error terms ($\epsilon$)

- We want a statistical learning method that simultaneously achieves
low variance and low bias.

The Bias-Variance Trade-Off: Variance
========================================================
- Variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\hat{f}$.

- If a method has high variance then small changes in the training data can result in large changes in $\hat{f}$. 

- In general, more flexible statistical methods have higher variance.

The Bias-Variance Trade-Off: Bias
========================================================
- Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much
simpler model.

- Generally, more flexible methods result in less bias.

- As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease.

The Bias-Variance Trade-Off: Equation
========================================================
The mentioned can be summarized with the following equation

$$E(y_0 - \hat{f}(x_0))^2 = Var\hat{f}(x_0) + [Bias\hat{f}(x_0)] + Var(\epsilon)$$

The Classification Setting
========================================================
- Many of the concepts that we have encountered, such as the bias-variance trade-off, can be transfer over to the classification setting with only some modifications due to the fact that $y_i$ is no longer numerical.

- The most common approach for quantifying the accuracy of our estimate $\hat{f}$ is the training error rate, the proportion of mistakes that are made if we apply our estimate $\hat{f}$ to the training observation by computing the fraction of incorrect classifications.

- A good classifier is one for which the test error, $Ave(I(y_0 \neq \hat{y}_0))$, is smallest. Refer to page 37.

- We want to assess the uncertainty in each classification and understand the roles of the different predictors among $X = (X_1, X_2, ..., X_p)$

The Bayes Classifier
========================================================
- The test error rate, $Ave(I(y_0 \neq \hat{y}_0))$, is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values.

- Bayes classifier is a conditional probability or the probability
that $Y = j$, given the observed predictor vector $x_0$.

- The Bayes classifier produces the lowest possible test error rate, called
the Bayes error rate.

K-Nearest Neighbors
========================================================
- in real data, we do not know the conditional distribution of $Y$ given $X$, and so computing the Bayes classifier is impossible.

- K-Nearest Neighbors: Given a positive integer $K$ and a test observation $x_0$, the KNN classifier first identifies the $K$ points in the training data that are closest to $x_0$ , represented by $N_0$.

- KNN applies Bayes rule and classifies the test observation $x_0$ to the class with the largest probability.

K-Nearest Neighbors
========================================================

- The choice of K has a drastic effect on the KNN classifier obtained.

- When $K = 1$, the decision boundary is overly flexible and finds patterns in the data that donâ€™t correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance.

- As $K$ grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.

The Classification Setting
========================================================

- Other types of Classification Methods are: Support Vector Machine, forms of Logistic and Linear Regression and Dyscriminant Analysis

