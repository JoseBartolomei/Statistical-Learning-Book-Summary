Introduction to Statistical Learning Book Summary
========================================================
author: 
date: 

Chapter 2
========================================================

Introduction
========================================================

- Statistical learning refers to a vast set of tools for understanding data.
- These tools can be classified as supervised or unsupervised.
	- Supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs.
	- Unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data.

Supervise Learning
========================================================
- There are two main reasons that we may wish to estimate $f$:
	- Prediction
	- Inference

Prediction
========================================================
- Prediction: We want to obtain the unknown value of an output ($Y$) using the known values of inputs ($X's$)

$$\hat{Y} = \hat{f}(X)$$

- where 
	- $\hat{f}(X)$ represents the function to estimate $Y$ given the values of $X$.
	- $\hat{Y}$ represents the resulting prediction or estimation of $Y$.

Figure. 1
========================================================
```{r librariesDAta, echo=FALSE, results='hide'}
library("ISLR"); library(MASS)
```

```{r scaterplot, echo=FALSE, fig.width=15, fig.height=10}
plot(medv~lstat, Boston, main = "Median value of owner-occupied homes in $1,000  (medv) vs. Lower status of the population (percent) (lstat).")
```

Prediction
========================================================
$$E(Y-\hat{Y})^2 = [f(X) - \hat{f}(X)^2] + Var(\epsilon)$$

- The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on two quantities,
	- reducible error: This error is reducible because we can potentially improve the accuracy of $\hat{f}$ by using the most appropriate statistical learning technique to estimate $f$.
	- irreducible error: Y is also a function of $\epsilon$, which, by definition, cannot be predicted using X. Therefore, variability associated with also affects the accuracy of our predictions.
		- No matter how well we estimate $f$, we cannot reduce the introduced error, $\epsilon$.

	
Model 1
========================================================
```{r lm1, echo=FALSE}
# First six values of Boston variables medv and lstat
head(Boston[, c("medv", "lstat")])
# Sinble linear model to predict new Y's.
lm1 <- lm(medv~lstat, Boston)
lm1
```

Prediction 1
========================================================
```{r newYpredict, echo=FALSE, results='hide'}
# New Y's 
newY <- data.frame(lstat = c(10, 20, 30, 40, 50))
pred1 <- predict(lm1, newdata = newY)
```
```{r predTable, echo=FALSE}
Ypred <- data.frame("New lstat" = newY, "medv Prediction" = pred1)
Ypred

lm1
```
#### Prediction Formula
$$ medv = 34.55 + (New lstat * -0.95)$$

Plot Model 1
========================================================
```{r PlotLm1, echo=FALSE, fig.width=15, fig.height=12}
plot(medv~lstat, Boston, main = "Median value of owner-occupied homes in $1,000  (medv) vs. Lower status of the population (percent) (lstat).",
		 cex = 3, col = "blue")

abline(lm1, col = "red", cex = 5)
text(x = 25, y = 35, labels = "medv = 34.55 + (New lstat * -0.95)",
		 cex = 3)
```

Error
========================================================
- The quantity $\epsilon$ may contain:
	- unmeasured variables that are useful in predicting $Y$.
	- unmeasured variations

```{r plotResid, echo=FALSE, fig.width=15, fig.height=10}
plot(medv~lstat, Boston, main = "Median value of owner-occupied homes in $1,000  (medv) vs. Lower status of the population (percent) (lstat).",
		 cex = 3, col = "blue")

abline(lm1, col = "red", cex = 5)
segments(x0 = Boston$lstat, y0 = Boston$medv,  y1 = lm1$fitted.values)

text(x = 25, y = 35, labels = "medv = 34.55 + (New lstat * -0.95)",
		 cex = 3)
```

Inference
========================================================
- In inference we want to understand the relationship between $X$ and $Y$, or more specifically, to understand how $Y$ changes as a function of $X_1 , . . . , X_p$.

- For inference we may ask:
	- Which predictors are associated with the response?
	- What is the relationship between the response and each predictor?
	- Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?

Inference
========================================================
Following the previous modeling example, can be said:

- The predictor lstat is negativelly associated with medv.
- For every unit increase in lstat there is .95 decrease in medv.
- When lstat is included alone in the model it relationship with medv is statistically significant.
- lstat predict the value of medv with a 54% accuracy (Adjusted R-Squared = 0.5432)
	- Meaning, there are other variables that should aid in the medv prediction.

How Do We Estimate f?
========================================================

$f$ is estimated by parametric or non-parametric functions

Parametric
========================================================
Parametric methods involve a two-step approach

1. Make an assumption about the functional form, or shape, of $f$ (eg., linear, quadratic, etc).

2. Find a procedure that uses the training data to fit or train the model (eg., ordinary least square, maximum likelihood, etc).

Non-Parametric
========================================================

- Non-parametric methods do not make explicit assumptions about the functional form of $f$. Instead they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly.

- Advantages: By avoiding the assumption of a particular functional form for $f$, they have the potential to accurately fit a wider range of possible shapes for $f$.  
	- Any parametric approach brings with it the possibility that the functional form used to estimate $f$ is very different from the true $f$, in which case the resulting model will not fit the data well. 
	- In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of $f$ is made.

Non-Parametric disadvantage
========================================================

Since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more
than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$.

The Trade-Off Between Prediction Accuracy and Model Interpretability
========================================================

- Some models are less flexible or restrictive, in the sense that they can produce just a relatively small range of shapes to estimate $f$.
- Flexible vs non-flexible
	- If interested in inference, then restrictive models are much more interpretable.
	- If prediction is of interest less interpretable but more flexible and accurate models are useful.
	
Supervised versus Unsupervised Learning
========================================================
- Supervised: For each observation of the predictor measurement(s) $x_i , i = 1, . . . , n$ there is an associated response measurement $y_i$ .
	- We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference).
	- examples: linear or logistic regression, Generalized Additive Model, boosting, support vector machines.

Supervised versus Unsupervised Learning
========================================================
- Unsupervised: For every observation $i = 1, . . . , n$, we observe a vector of measurements $x_i$ but no associated response $y_i$.
- Lack a response variable that can supervise our analysis.
- With this analysis is possible to understand the relationship between variables or between the observations.
	- examples: cluster analysis, clustering

Regression Versus Classification Problems
========================================================
- Regression: Usually used when the response is quantitative
- Classification: Usually used when the response is qualitative

However, the distinction is not always that crisp at the moment of select a statistical methods because some models can be extended to work with both, quantitative and qualitative response.

Assessing Model Accuracy: Measuring the Quality of Fit
========================================================
When assessing the quality of the fit we want to:

1. measure how well its predictions actually match the observed data.
2. quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation.
3. The most common measure is the mean squared error (MSE)

$$MSE = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{f}(x_i))^2$$

MSE Steps
========================================================
1. Obtain the Information Matrix

<center>

Obs.  | $Y$		| $\hat{Y}$	  | $Y - \hat{Y}$
----- | ----- | ----------- | -------------
1  		| $Y_1$	|	$\hat{Y}_1$	| $Y_1 - \hat{Y}_1$
2  		| $Y_2$	|	$\hat{Y}_2$	| $Y_2 - \hat{Y}_2$
3  		| $Y_3$	|	$\hat{Y}_3$	| $Y_2 - \hat{Y}_2$
... 	| $Y_{...}$	|$\hat{Y}_{...}$ | $Y_{...} - \hat{Y}_{...}$
n  		| $Y_n$	|$\hat{Y}_n$	| $Y_n - \hat{Y}_n$
</center>

MSE Steps (continuation)
========================================================
2. Sum the differences between the observed and predicted values = $Z$

$$\sum_{i=1}^n(Y_i-\hat{Y_i}) = Z$$

3. Squared $Z$ and divided it over the total number of observations ($n$)

$$\frac{Z^2}{n} = MSE$$


Train Data MSE on lm1
========================================================
```{r MSE(lm1)}
(sum((Boston$medv - predict(lm1))^2)) / nrow(Boston)
```

# Or

```{r mseFunct}
mse <- function(sm) { 
    mse <- mean(sm$residuals^2)
    return(mse)
}
mse(lm1)
```

MSE
========================================================
The MSE will be **small** if the predicted responses are very close to the true responses, and will be **large** if for some of the observations, the predicted and true responses differ substantially.

- We do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.

- We want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE.

- If test data is available, implement learning methods to the test data and select the model with the small value of MSE.

MSE
========================================================

- **Degrees of freedom** is the average training MSE as a function of flexibility.
	- The degrees of freedom is a quantity that summarizes the flexibility of a curve.

-  As model flexibility increases, training MSE will decrease, but the test MSE may not.

- This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. 
- When a given method yields a small training MSE but a large test MSE, we are said to be over-fitting the data.

MSE
========================================================
- This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function $f$.

- When we over-fit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data.

- Over-fitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.

Over-fitting model
========================================================

```{r}
lm1 <- lm(medv ~ lstat, data = Boston)
lm2 <- lm(medv ~ lstat + rm + dis + tax, data = Boston)
lm3 <- lm(medv ~ lstat + I(lstat^2) + rm + I(rm^2) + dis + tax, data = Boston)

mse(lm1); mse(lm2); mse(lm3)
```
Mean Square Error for fitted models
========================================================

```{r mse.df, echo=FALSE}
mse.df <- data.frame(model = c(1,2,3), mse = c(mse(lm1), mse(lm2), mse(lm3)))
```

```{r plot, echo=FALSE, fig.width=15, fig.height=10}
plot(mse.df$model, mse.df$mse, type = "o", col = "red", lwd = 4,
		 main = "Mean Square Error for fitted models")

```

train and test mse plot --- check test mse method 
========================================================
```{r echo=FALSE, results='hide'}
library("ISLR"); library(MASS)

dim(Boston)

set.seed(100)
train <- Boston[sample(nrow(Boston), size = nrow(Boston)/2),]
dim(train)

test <- Boston[!(rownames(Boston) %in% rownames(train)),]
dim(test)

table(rownames(train) == rownames(test))

#model on train data
lm1 <- lm(medv ~ lstat, data = train)
lm2 <- lm(medv ~ lstat + rm + dis + tax, data = train)
lm3 <- lm(medv ~ lstat + I(lstat^2) + rm + I(rm^2) + dis + tax, data = train)


mse(lm1); mse(lm2); mse(lm3)

# predict on test data

tm1 <- lm(medv ~ lstat, data = test)
tm2 <- lm(medv ~ lstat + rm + dis + tax, data = test)
tm3 <- lm(medv ~ lstat + I(lstat^2) + rm + I(rm^2) + dis + tax, data = test)

### Check the Adequacy of using the mse function on the test data #####
mse(tm1); mse(tm2); mse(tm3)

mse.df <- data.frame(model = c(1,2,3), mse_train = c(mse(lm1), mse(lm2), mse(lm3)),
										 mse_test = c(mse(tm1), mse(tm2), mse(tm3))
)
mse.df
```
```{r plotTTmse, echo=FALSE, fig.width=15, fig.height=10}
plot(mse.df$model, mse.df$mse_train, type = "o", col = "red", lwd = 4,
		 main = "Mean Square Error for fitted models", ylim=c(ymin = 10, ymax = 40)
)
text(x = 2, y = 35, "train mse")

points(mse.df$model, mse.df$mse_test)
lines(mse.df$model, mse.df$mse_test, col = "green", lwd = 4)
text(x = 2, y = 20, "test mse")

```

The Bias-Variance Trade-Off
========================================================
- The U-shape usually observed in a test MSE curves turns out to be the result of two competing properties of statistical learning methods: Bias and Variance.

- Expected test MSE, for a given value $x_0$, can always be decomposed into the sum of three fundamental quantities: 
	- the variance of $\hat{f}(x_0)$,
	- the squared bias of $\hat{f}(x_0)$
	- the variance of the error terms ($\epsilon$)

- We want a statistical learning method that simultaneously achieves
low variance and low bias.

The Bias-Variance Trade-Off: Variance
========================================================
- Variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\hat{f}$.

- If a method has high variance then small changes in the training data can result in large changes in $\hat{f}$. 

- In general, more flexible statistical methods have higher variance.

The Bias-Variance Trade-Off: Bias
========================================================
- Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much
simpler model.

- Generally, more flexible methods result in less bias.

- As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease.

The Bias-Variance Trade-Off: Equation
========================================================
The mentioned can be summarized with the following equation

$$E(y_0 - \hat{f}(x_0))^2 = Var\hat{f}(x_0) + [Bias(\hat{f}(x_0))]^2 + Var(\epsilon)$$

The Classification Setting
========================================================
- Many of the concepts that we have encountered, such as the bias-variance trade-off, can be transfer over to the classification setting with only some modifications due to the fact that $y_i$ is no longer numerical.

- The most common approach for quantifying the accuracy of our estimate $\hat{f}$ is the training error rate, the proportion of mistakes that are made if we apply our estimate $\hat{f}$ to the training observation by computing the fraction of incorrect classifications.

- A good classifier is one for which the test error, $Ave(I(y_0 \neq \hat{y}_0))$, is smallest. Refer to page 37.

- We want to assess the uncertainty in each classification and understand the roles of the different predictors among $X = (X_1, X_2, ..., X_p)$

The Bayes Classifier
========================================================
- The test error rate, $Ave(I(y_0 \neq \hat{y}_0))$, is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values.

- Bayes classifier is a conditional probability or the probability
that $Y = j$, given the observed predictor vector $x_0$.

- The Bayes classifier produces the lowest possible test error rate, called
the Bayes error rate.

K-Nearest Neighbors
========================================================
- In real data, we do not know the conditional distribution of $Y$ given $X$, and so computing the Bayes classifier is impossible.

- K-Nearest Neighbors: Given a positive integer $K$ and a test observation $x_0$, the KNN classifier first identifies the $K$ points in the training data that are closest to $x_0$ , represented by $N_0$.

- KNN applies Bayes rule and classifies the test observation $x_0$ to the class with the largest probability.

K-Nearest Neighbors
========================================================

- The choice of K has a drastic effect on the KNN classifier obtained.

- When $K = 1$, the decision boundary is overly flexible and finds patterns in the data that don’t correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance.

- As $K$ grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.

The Classification Setting
========================================================

- Other types of Classification Methods are: Support Vector Machine, forms of Logistic and Linear Regression and Discriminant Analysis

